# visualize MCMC samples in data/posterior_samps/param_samples.csv
library(ggplot2)
library(GGally)
#' A Gelman-Rubin convergence diagnostic function.
#'
#' This function calculates the Gelman-Rubin convergence diagnostic (Rhat) on some samples.
#' It assumes each chain is in a separate csv file, and each file contains the samples for
#' all parameters. For each chain, this function splits each column into two, which means
#' "m" comes out to be twice the number of csv files. Finally, this function assumes each
#' csv has a one-line header.
#' @param chain_files vector of string paths for each file of samples
#' @param burn the number of rows you want to discard as burn in. Defaults to 0.
#' @param ... arguments to be passed in to lapply
#' @keywords rhat Gelman-Rubin
#' @export
#' @examples
#' files <- c('/fake/path/samps1.csv', '/fake/path/samps2.csv')
#' rhats(files, burn = 300)
rhats <- function(chain_files, burn = 0, ...){
# TODO: check all files are hte same shape
# TODO: print the first couple of rows to make sure they look ok
if(burn > 0){
dfs <- lapply(chain_files,
function(path) read.csv(path, header=T)[-1:-burn,],
...)
}else{
dfs <- lapply(chain_files,
function(path) read.csv(path, header=T),
...)
}
only_one_column <- is.null(nrow(dfs[[1]]))
m <- 2*length(chain_files)
if(only_one_column){
n <- length(dfs[[1]])
}else {
n <- nrow(dfs[[1]])
}
n <- (n %/% 2)*2 # in case n is odd
if(only_one_column){
dfs_halved <- lapply(dfs, function(df) df[((n/2+1):n)])
dfs_halved <- c(dfs_halved, lapply(dfs, function(df) df[(1:(n/2))]))
}else {
dfs_halved <- lapply(dfs, function(df) df[((n/2+1):n),])
dfs_halved <- c(dfs_halved, lapply(dfs, function(df) df[(1:(n/2)),]))
}
n <- n/2
if(only_one_column){
num_params <- 1
}else {
num_params <- ncol(dfs_halved[[1]])
}
rm(dfs)
# calculate all rhat statistics for each untransformed univariate parameter
rhats <- vector(mode="numeric", length = num_params)
for(param_num in 1:num_params){
if(only_one_column)
univariate_chains <- dfs_halved
else
univariate_chains <- sapply(dfs_halved, '[', param_num) # pick out the same col from each df
psi_bar_dot_js <- sapply(univariate_chains, mean)
psi_bar_dot_dot <- mean(psi_bar_dot_js)
ss_j <- sapply(univariate_chains, var)
w <- mean(ss_j)
b <- var(psi_bar_dot_js)*n
varhat_plus <- w*(n-1)/n + b/n
rhats[param_num] <- sqrt(varhat_plus/w)
}
return(rhats)
}
#' A acceptance rate function.
#'
#' This function calculates the acceptance rate for samples stored in a data frame.
#' @param df A data frame of samples. Rows correspond with iterations, and columns with parameters.
#' @param burn Number of rows you want to discard as burn in. Defaults to 0.
#' @keywords accept acceptance rate
#' @export
#' @examples
#' fake_samples <- data.frame(rnorm(100))
#' acceptRate(fake_samples)
acceptRate <- function(df, burn=0){
start <- burn + 1
mean(abs(diff(df[start:nrow(df),1])) > .000000001)
}
setwd("~/jsm22_pp/")
burn <- 100
num_lw_particles <- 500
# 1. read in data sets
# MCMC samps are stored in phi mu sigmaSquared, rho
# liu-west samps are stored in phi, mu, sigma, rho
d <- read.csv("data/posterior_samps/param_samples.csv", header=F)
d[,3] <- sqrt(d[,3])
colnames(d) <- c("phi","mu","sigmaSquared","rho")
d$iter <- 1:nrow(d)
d <- d[-(1:burn),]
all_lw_post <- read.csv("data/posterior_samps/lw_aux_posterior.txt", header=F, sep = "")
all_lw2_post <- read.csv("data/posterior_samps/lw2_prior_posterior.txt", header=F, sep = "")
head(all_lw_post)
hist(all_lw_post[,3])
hist(all_lw_post[,3]^2)
setwd("~/jsm22_pp/")
burn <- 100
num_lw_particles <- 500
# 1. read in data sets
# MCMC samps are stored in phi mu sigmaSquared, rho
# liu-west samps are stored in phi, mu, sigma, rho
d <- read.csv("data/posterior_samps/param_samples.csv", header=F)
d[,3] <- sqrt(d[,3])
colnames(d) <- c("phi","mu","sigmaSq","rho")
d$iter <- 1:nrow(d)
d <- d[-(1:burn),]
quantile(d[,1])
quantile(d[,1], probs = c(.025, .975))
quantile(d[,2], probs = c(.025, .975))
quantile(sqrt(d[,3]), probs = c(.025, .975))
quantile(d[,4], probs = c(.025, .975))
# changes made here will be propogated to many files
phiLow <- 0.864248
phiHigh <- 0.972859
muLow <- -0.390150
muHigh <- 0.551337
sigmaLow <- 0.4495658
sigmaHigh <- 0.6868961
rhoLow <- -0.955950
rhoHigh <- -0.522117
dte <- 5
delta <- .99
paramSamplesFile <- "data/posterior_samps/param_samples.csv"
estDataFile <- "data/SPY_returns_estimation.csv"
numMCMCIters <- 100000
numPFs <- 7
burn <- 100
setwd("~/jsm22_pp/data/")
## check your highs and lows are consistent with the data
d <- read.csv("posterior_samps/param_samples.csv", header=F)
parEstimates <- colMeans(d[(burn+1):nrow(d),])
phiGood <- (phiLow < parEstimates[1]) & (parEstimates[1] < phiHigh)
muGood <- (muLow < parEstimates[2]) & (parEstimates[2] < muHigh)
sigmaGood <- (sigmaLow^2 < parEstimates[3]) & (parEstimates[3] < sigmaHigh^2)
rhoGood <- (rhoLow < parEstimates[4]) & (parEstimates[4] < rhoHigh)
phiGood
muGood
sigmaGood
rhoGood
parEstimates[3]
sigmaLow^2
# changes made here will be propogated to many files
phiLow <- 0.864248
phiHigh <- 0.972859
muLow <- -0.390150
muHigh <- 0.551337
sigmaLow <- 0.4495658
sigmaHigh <- 0.6868961
rhoLow <- -0.955950
rhoHigh <- -0.522117
dte <- 5
delta <- .99
paramSamplesFile <- "data/posterior_samps/param_samples.csv"
estDataFile <- "data/SPY_returns_estimation.csv"
numMCMCIters <- 100000
numPFs <- 7
burn <- 100
setwd("~/jsm22_pp/data/")
## check your highs and lows are consistent with the data
d <- read.csv("posterior_samps/param_samples.csv", header=F)
d[,3] <- sqrt(d[,3])
parEstimates <- colMeans(d[(burn+1):nrow(d),])
phiGood <- (phiLow < parEstimates[1]) & (parEstimates[1] < phiHigh)
muGood <- (muLow < parEstimates[2]) & (parEstimates[2] < muHigh)
sigmaGood <- (sigmaLow < parEstimates[3]) & (parEstimates[3] < sigmaHigh)
rhoGood <- (rhoLow < parEstimates[4]) & (parEstimates[4] < rhoHigh)
sigmaGood
hist(d[,3])
parEstimates[,3]
parEstimates[3]
summary(d[,3])
parEstimates[,3]
parEstimates[3]
## check your highs and lows are consistent with the data
d <- read.csv("posterior_samps/param_samples.csv", header=F)
hist(d[,3])
max(d[,3])
summary(d[,3])
## check your highs and lows are consistent with the data
d <- read.csv("posterior_samps/param_samples.csv", header=F)
d[,3] <- sqrt(d[,3])
summary(d[,3])
quantile(d[,3], c(.025,.975))
# changes made here will be propogated to many files
phiLow <- 0.864248
phiHigh <- 0.972859
muLow <- -0.390150
muHigh <- 0.551337
sigmaLow <- 0.2021094
sigmaHigh <- 0.4718427
rhoLow <- -0.955950
rhoHigh <- -0.522117
dte <- 5
delta <- .99
paramSamplesFile <- "data/posterior_samps/param_samples.csv"
estDataFile <- "data/SPY_returns_estimation.csv"
numMCMCIters <- 100000
numPFs <- 7
burn <- 100
setwd("~/jsm22_pp/data/")
## check your highs and lows are consistent with the data
d <- read.csv("posterior_samps/param_samples.csv", header=F)
d[,3] <- sqrt(d[,3])
parEstimates <- colMeans(d[(burn+1):nrow(d),])
phiGood <- (phiLow < parEstimates[1]) & (parEstimates[1] < phiHigh)
muGood <- (muLow < parEstimates[2]) & (parEstimates[2] < muHigh)
sigmaGood <- (sigmaLow < parEstimates[3]) & (parEstimates[3] < sigmaHigh)
rhoGood <- (rhoLow < parEstimates[4]) & (parEstimates[4] < rhoHigh)
stopifnot(phiGood & muGood & sigmaGood & rhoGood)
